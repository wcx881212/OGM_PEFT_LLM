{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "layer1 = nn.Linear(8,3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1.]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((3,8))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "8"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1.weight.size(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[-0.0581,  0.0361, -0.0649,  0.0933],\n         [ 0.2731, -0.2574, -0.2784, -0.2805],\n         [ 0.2419,  0.2746,  0.2415,  0.3205]], grad_fn=<SliceBackward0>),\n tensor([[-0.3356, -0.2775,  0.2027,  0.0922],\n         [-0.1183,  0.1392, -0.2094,  0.2146],\n         [-0.0479,  0.0785,  0.1698, -0.1927]], grad_fn=<SliceBackward0>))"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1.weight[:,4:], layer1.weight[:,:4]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 1., 1., 0., 0., 0., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0., 0., 0., 0.]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones((2,3))\n",
    "y = torch.zeros((2,6))\n",
    "z = torch.cat((x,y),dim=1)\n",
    "z"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([18, 10]), torch.Size([18, 10]))"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones((18,20))\n",
    "gama, beta = torch.split(x,10,1)\n",
    "gama.shape, beta.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294912 || all params: 109777152 || trainable%: 0.2686460658042941\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import BertModel, BertConfig\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "bertconfig = BertConfig.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    ")\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased', config=bertconfig)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "# trainable params: 294912 || all params: 109777152 || trainable%: 0.2686460658042941"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1056, 28394,  2102,  3793,  1024,  1030, 11990,  7874,  3215,\n",
      "         25465,  2115,  8013,  2326,  2038,  2042,  9202,  2076,  1996,  9131,\n",
      "          2832,  1012,  1045,  2097,  2196,  5309,  1037, 11990,  2153,  1012,\n",
      "          3830,  1024,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "bertTokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "inputs = bertTokenizer.encode_plus(\"Tweet text : @HondaCustSvc Your customer service has been horrible during the recall process. I will never purchase a Honda again. Label :\", return_tensors=\"pt\",max_length=512,padding=True,truncation=True)\n",
    "print(inputs)\n",
    "#input = [101, 1045, 2572, 2058, 20192, 2102, 102]\n",
    "# x = model.get_base_model().forward(inputs[\"input_ids\"])\n",
    "# print(model())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [39]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minput_ids\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtoken_type_ids\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mattention_mask\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32mD:\\Conada\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\Conada\\lib\\site-packages\\peft\\peft_model.py:823\u001B[0m, in \u001B[0;36mPeftModelForTokenClassification.forward\u001B[1;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, **kwargs)\u001B[0m\n\u001B[0;32m    820\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m    822\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpeft_config, PromptLearningConfig):\n\u001B[1;32m--> 823\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_model(\n\u001B[0;32m    824\u001B[0m         input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[0;32m    825\u001B[0m         attention_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[0;32m    826\u001B[0m         inputs_embeds\u001B[38;5;241m=\u001B[39minputs_embeds,\n\u001B[0;32m    827\u001B[0m         labels\u001B[38;5;241m=\u001B[39mlabels,\n\u001B[0;32m    828\u001B[0m         output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n\u001B[0;32m    829\u001B[0m         output_hidden_states\u001B[38;5;241m=\u001B[39moutput_hidden_states,\n\u001B[0;32m    830\u001B[0m         return_dict\u001B[38;5;241m=\u001B[39mreturn_dict,\n\u001B[0;32m    831\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    832\u001B[0m     )\n\u001B[0;32m    834\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m input_ids\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    835\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    836\u001B[0m     \u001B[38;5;66;03m# concat prompt attention mask\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Conada\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "\u001B[1;31mTypeError\u001B[0m: forward() got an unexpected keyword argument 'labels'"
     ]
    }
   ],
   "source": [
    "print(model(inputs['input_ids'],inputs['token_type_ids'],inputs['attention_mask']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 1.1691e-01, -3.9735e-02, -1.0928e-01,  ..., -3.6141e-01,\n",
      "           2.6877e-01,  3.3045e-01],\n",
      "         [ 1.2260e+00, -7.4663e-01,  3.6256e-01,  ...,  1.6007e-01,\n",
      "           7.5153e-01,  2.7402e-01],\n",
      "         [ 1.3149e+00, -2.8815e-01,  4.9387e-01,  ..., -6.4974e-01,\n",
      "          -7.7650e-02, -1.9502e-01],\n",
      "         ...,\n",
      "         [ 1.2370e-01, -9.5042e-01,  3.6132e-01,  ..., -1.1498e-01,\n",
      "          -1.0193e-01, -2.3024e-01],\n",
      "         [-1.2057e-01,  2.0980e-01, -2.9318e-01,  ..., -3.8923e-01,\n",
      "           3.7410e-01,  4.9506e-01],\n",
      "         [ 4.3562e-01,  3.0449e-01,  4.8004e-05,  ...,  6.2245e-02,\n",
      "          -7.3993e-01, -4.2453e-01]]], grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.7736, -0.5322, -0.9692,  0.6489,  0.7484, -0.1469,  0.4637,  0.3117,\n",
      "         -0.8825, -1.0000, -0.4685,  0.9510,  0.9598,  0.7588,  0.8338, -0.4510,\n",
      "          0.0442, -0.5428,  0.3221,  0.4124,  0.6666,  1.0000, -0.0673,  0.2832,\n",
      "          0.5612,  0.9906, -0.6206,  0.8610,  0.9375,  0.6137, -0.3782,  0.2319,\n",
      "         -0.9806, -0.3329, -0.9809, -0.9889,  0.5267, -0.5686,  0.0714,  0.0442,\n",
      "         -0.7978,  0.2984,  1.0000,  0.3780,  0.5621, -0.2018, -1.0000,  0.3202,\n",
      "         -0.7415,  0.9656,  0.8911,  0.9660,  0.2313,  0.4252,  0.4975, -0.5846,\n",
      "         -0.0195,  0.0758, -0.3218, -0.5566, -0.6591,  0.5025, -0.9315, -0.8628,\n",
      "          0.8887,  0.9445, -0.2676, -0.3251, -0.1459, -0.1451,  0.6492,  0.2629,\n",
      "         -0.2804, -0.8511,  0.7897,  0.2915, -0.7621,  1.0000, -0.4164, -0.9564,\n",
      "          0.9420,  0.8597,  0.6994, -0.4854,  0.6832, -1.0000,  0.5122, -0.0043,\n",
      "         -0.9783,  0.3595,  0.6395, -0.2002,  0.8479,  0.6648, -0.5622, -0.5863,\n",
      "         -0.3483, -0.8896, -0.4069, -0.4643,  0.2924, -0.1955, -0.4512, -0.3641,\n",
      "          0.4380, -0.5562, -0.1437,  0.7551,  0.1559,  0.6345,  0.5877, -0.4531,\n",
      "          0.4434, -0.9141,  0.6553, -0.4356, -0.9780, -0.7264, -0.9781,  0.6229,\n",
      "         -0.2344, -0.2889,  0.8786, -0.6283,  0.3881, -0.1258, -0.9661, -1.0000,\n",
      "         -0.6217, -0.5596, -0.3403, -0.2398, -0.9545, -0.9494,  0.6105,  0.9218,\n",
      "          0.2752,  0.9999, -0.3359,  0.9075, -0.4604, -0.7223,  0.6667, -0.5859,\n",
      "          0.8230, -0.2652, -0.2901,  0.1202, -0.4377,  0.4689, -0.7338, -0.1763,\n",
      "         -0.8299, -0.8157, -0.4442,  0.8887, -0.6389, -0.9767, -0.2715, -0.2153,\n",
      "         -0.2042,  0.7385,  0.6802,  0.3862, -0.4458,  0.5764,  0.2923,  0.4960,\n",
      "         -0.5478, -0.2124,  0.3748, -0.3709, -0.9492, -0.9692, -0.3779,  0.3854,\n",
      "          0.9754,  0.3886,  0.3821,  0.8686, -0.3621,  0.7978, -0.9460,  0.9640,\n",
      "         -0.2232,  0.3923, -0.8432,  0.7315, -0.6112,  0.2720,  0.6345, -0.7544,\n",
      "         -0.6735, -0.2244, -0.5195, -0.3912, -0.8854,  0.1167, -0.2889, -0.3936,\n",
      "         -0.2225,  0.8866,  0.8200,  0.3942,  0.5707,  0.6010, -0.7408, -0.4583,\n",
      "         -0.0259,  0.2377,  0.1815,  0.9818, -0.8375, -0.0991, -0.8624, -0.9770,\n",
      "         -0.1788, -0.8007, -0.3213, -0.6565,  0.7100, -0.6629,  0.5370,  0.3694,\n",
      "         -0.7819, -0.6478,  0.3621, -0.5715,  0.4872, -0.2075,  0.9486,  0.9791,\n",
      "         -0.6900, -0.1383,  0.9509, -0.9559, -0.7237,  0.3962, -0.2706,  0.6801,\n",
      "         -0.6920,  0.9574,  0.9622,  0.7434, -0.7837, -0.9016, -0.3270, -0.7872,\n",
      "         -0.0870,  0.3431,  0.9127,  0.7557,  0.4437,  0.2529, -0.4530,  0.8792,\n",
      "         -0.9270, -0.9165, -0.9125, -0.1929, -0.9796,  0.8878,  0.3095,  0.5486,\n",
      "         -0.4983, -0.5731, -0.9166,  0.4940,  0.0878,  0.9102, -0.5619, -0.7171,\n",
      "         -0.6162, -0.8924, -0.1117, -0.3970, -0.5491, -0.0978, -0.8138,  0.4869,\n",
      "          0.3937,  0.5642, -0.9590,  0.9864,  1.0000,  0.9435,  0.8029,  0.6382,\n",
      "         -1.0000, -0.8511,  1.0000, -0.9950, -1.0000, -0.7506, -0.5373,  0.1099,\n",
      "         -1.0000, -0.2631,  0.0931, -0.8359,  0.7244,  0.9431,  0.8749, -1.0000,\n",
      "          0.5511,  0.8059, -0.7523,  0.9684, -0.4568,  0.9136,  0.4807,  0.4668,\n",
      "         -0.2550,  0.4499, -0.9699, -0.7694, -0.7094, -0.8135,  0.9998,  0.1011,\n",
      "         -0.7345, -0.7722,  0.5948, -0.1411, -0.0790, -0.9357, -0.3477,  0.5946,\n",
      "          0.7376,  0.2097,  0.3512, -0.3296,  0.2831,  0.3888, -0.1197,  0.7438,\n",
      "         -0.8858, -0.0391, -0.4932,  0.0011, -0.7478, -0.9648,  0.9388, -0.4667,\n",
      "          0.9345,  1.0000,  0.5252, -0.5857,  0.6465,  0.2754, -0.6458,  1.0000,\n",
      "          0.8872, -0.9569, -0.7489,  0.7375, -0.5985, -0.7421,  0.9989, -0.2832,\n",
      "         -0.8199, -0.2384,  0.9715, -0.9757,  0.9992, -0.7669, -0.9469,  0.9158,\n",
      "          0.8878, -0.5610, -0.4717,  0.1936, -0.7869,  0.2450, -0.6562,  0.6044,\n",
      "          0.3600,  0.0077,  0.8012, -0.2493, -0.7226,  0.2450, -0.7326, -0.2560,\n",
      "          0.9731,  0.5632, -0.2663,  0.0746, -0.3371, -0.8212, -0.9544,  0.6912,\n",
      "          1.0000, -0.2490,  0.8906, -0.3469, -0.1025,  0.0440,  0.5670,  0.5883,\n",
      "         -0.3037, -0.7056,  0.8725, -0.7599, -0.9787,  0.4578,  0.1863, -0.3231,\n",
      "          1.0000,  0.4252,  0.3253,  0.5590,  0.9883,  0.0301,  0.2305,  0.9455,\n",
      "          0.9559, -0.2766,  0.7231,  0.2897, -0.9355, -0.4242, -0.6026,  0.1112,\n",
      "         -0.9280, -0.0996, -0.8726,  0.9296,  0.9855,  0.4513,  0.3207,  0.8043,\n",
      "          1.0000, -0.9080,  0.4041,  0.8168,  0.1263, -1.0000, -0.6357, -0.4154,\n",
      "         -0.0252, -0.8993, -0.3594,  0.3122, -0.9116,  0.9176,  0.8162, -0.8904,\n",
      "         -0.9742, -0.6511,  0.3316,  0.1089, -0.9961, -0.7347, -0.5817,  0.4419,\n",
      "         -0.2315, -0.8094, -0.4868, -0.3504,  0.5733, -0.2828,  0.7000,  0.9473,\n",
      "          0.7398, -0.9171, -0.2238, -0.2014, -0.6365,  0.6012, -0.6312, -0.9586,\n",
      "         -0.2061,  1.0000, -0.5106,  0.9344,  0.4369,  0.3083, -0.3376,  0.2693,\n",
      "          0.9787,  0.3413, -0.8104, -0.9313,  0.8036, -0.4780,  0.5850,  0.7482,\n",
      "          0.8430,  0.6562,  0.8712,  0.1513,  0.0591,  0.0099,  0.9812, -0.2603,\n",
      "         -0.2508, -0.4260, -0.0715, -0.4275,  0.5576,  1.0000,  0.2730,  0.5548,\n",
      "         -0.9793, -0.8982, -0.7133,  1.0000,  0.8276, -0.3181,  0.6775,  0.6363,\n",
      "         -0.2708,  0.2990, -0.1713, -0.3134,  0.3379,  0.1510,  0.8843, -0.6033,\n",
      "         -0.9505, -0.5063,  0.4441, -0.9088,  1.0000, -0.5782, -0.2645, -0.3410,\n",
      "         -0.2530, -0.9388, -0.0365, -0.9584, -0.3533,  0.2960,  0.8789,  0.3077,\n",
      "         -0.7525, -0.7301,  0.9014,  0.7312, -0.9367, -0.9317,  0.8870, -0.9685,\n",
      "          0.6336,  1.0000,  0.4505,  0.3529,  0.1913, -0.1429,  0.4572, -0.3603,\n",
      "          0.6296, -0.9011, -0.3667, -0.2046,  0.4958, -0.1826, -0.6359,  0.4984,\n",
      "          0.3597, -0.6946, -0.6778, -0.1559,  0.4234,  0.7757, -0.3955, -0.1996,\n",
      "          0.1235, -0.1314, -0.8341, -0.3055, -0.5391, -1.0000,  0.4979, -1.0000,\n",
      "          0.6648,  0.4150, -0.2382,  0.7405,  0.6789,  0.7899, -0.5761, -0.9298,\n",
      "          0.3923,  0.7189, -0.4015, -0.6023, -0.3293,  0.3719, -0.1045,  0.1261,\n",
      "         -0.8080,  0.7508, -0.2641,  1.0000,  0.1705, -0.5975, -0.8486,  0.2093,\n",
      "         -0.2922,  1.0000, -0.2677, -0.9172,  0.4834, -0.8037, -0.7897,  0.4882,\n",
      "          0.0253, -0.7514, -0.9689,  0.8622,  0.2969, -0.7368,  0.5888, -0.3423,\n",
      "         -0.4419,  0.1380,  0.9517,  0.9770,  0.5966,  0.6534, -0.7150, -0.5634,\n",
      "          0.9512,  0.3226, -0.5766,  0.1215,  1.0000,  0.3231, -0.8696, -0.0258,\n",
      "         -0.8354, -0.2771, -0.8391,  0.3135,  0.3110,  0.8431, -0.2244,  0.8898,\n",
      "         -0.9199,  0.0305, -0.5313, -0.7522,  0.4241, -0.8511, -0.9713, -0.9712,\n",
      "          0.7272, -0.4463, -0.1628,  0.3167,  0.1792,  0.4941,  0.4710, -1.0000,\n",
      "          0.9068,  0.4347,  0.9531,  0.8996,  0.8028,  0.6256,  0.3603, -0.9545,\n",
      "         -0.8292, -0.3846, -0.1854,  0.4884,  0.5760,  0.8109,  0.3981, -0.4706,\n",
      "         -0.5661, -0.7505, -0.8992, -0.9842,  0.4536, -0.7404, -0.6533,  0.9386,\n",
      "          0.1477, -0.1781, -0.5102, -0.9282,  0.4379,  0.6996,  0.0351,  0.0313,\n",
      "          0.2828,  0.7503,  0.8178,  0.9595, -0.9379,  0.5462, -0.8589,  0.4569,\n",
      "          0.9199, -0.9083,  0.2167,  0.6329, -0.4876,  0.2788, -0.3850, -0.7344,\n",
      "          0.7627, -0.4328,  0.3993, -0.4982,  0.0315, -0.4694, -0.2117, -0.5549,\n",
      "         -0.7170,  0.7785,  0.0953,  0.7996,  0.8936, -0.1507, -0.6968, -0.2590,\n",
      "         -0.8167, -0.8624,  0.4631, -0.1056, -0.5389,  0.7925, -0.1202,  0.9340,\n",
      "          0.3282, -0.3491, -0.3521, -0.3727,  0.7227, -0.6747, -0.5956, -0.4771,\n",
      "          0.6130,  0.2988,  1.0000, -0.8279, -0.9383, -0.4118, -0.4833,  0.5463,\n",
      "         -0.4539, -1.0000,  0.3667, -0.7424,  0.8534, -0.6570,  0.8904, -0.7998,\n",
      "         -0.9200, -0.3285,  0.6907,  0.8341, -0.5157, -0.6287,  0.7089, -0.6641,\n",
      "          0.9720,  0.7348, -0.4006,  0.1334,  0.7590, -0.8431, -0.6404,  0.7730]],\n",
      "       grad_fn=<TanhBackward0>), hidden_states=(tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
      "         [ 1.0329,  0.3579,  0.0808,  ..., -0.2105,  0.6121,  0.1687],\n",
      "         [ 0.0168, -0.9628, -0.3899,  ...,  1.0481, -0.3960, -0.7002],\n",
      "         ...,\n",
      "         [-0.6678, -0.1658,  1.1389,  ..., -0.3496,  0.3352, -0.4760],\n",
      "         [-0.7233,  0.4668, -0.3978,  ...,  0.2886,  0.1278,  0.6445],\n",
      "         [-0.5045, -0.2296,  0.3398,  ..., -0.7216,  0.2336, -0.2148]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0442,  0.0692, -0.1791,  ...,  0.2944, -0.0395, -0.0461],\n",
      "         [ 1.3605, -0.5083, -0.3526,  ...,  0.0912,  0.7500,  0.2341],\n",
      "         [ 0.4535, -1.5361, -0.3106,  ...,  1.0509,  0.5710, -1.1673],\n",
      "         ...,\n",
      "         [-0.2447, -0.3874,  1.4250,  ..., -0.2275,  0.1630, -0.3433],\n",
      "         [-1.0224,  0.2010, -0.2937,  ...,  0.4318, -0.2406,  0.3573],\n",
      "         [-0.4297, -0.2794,  0.1996,  ..., -0.3814,  0.2141, -0.2649]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0688, -0.2850, -0.3710,  ...,  0.4417,  0.1028, -0.0340],\n",
      "         [ 1.5343, -0.3461, -0.0324,  ..., -0.0705,  1.4462,  0.0460],\n",
      "         [ 0.6632, -1.6222,  0.1773,  ...,  0.5092,  0.6353, -1.6999],\n",
      "         ...,\n",
      "         [-0.3212, -0.2852,  2.0610,  ...,  0.3131,  0.2960, -0.3513],\n",
      "         [-0.8434,  0.3840,  0.2750,  ...,  0.6297, -0.2115,  0.2331],\n",
      "         [-0.3447, -0.3219,  0.1509,  ..., -0.1618,  0.2232, -0.3995]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0228, -0.3460, -0.1585,  ...,  0.3855,  0.2601,  0.1114],\n",
      "         [ 1.1399, -1.0918,  0.7665,  ...,  0.2717,  1.4972, -0.1192],\n",
      "         [ 1.1148, -1.1636,  1.0627,  ...,  0.5837,  1.0424, -1.8710],\n",
      "         ...,\n",
      "         [-0.2268, -0.2620,  1.9023,  ...,  0.2482,  0.1503, -0.3009],\n",
      "         [-0.6056,  0.4895,  0.4519,  ...,  0.1164, -0.1681, -0.3051],\n",
      "         [-0.0887, -0.1138,  0.1001,  ...,  0.0163,  0.0730, -0.0602]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0505, -0.7672, -0.4981,  ...,  0.2824,  0.4025,  0.6022],\n",
      "         [ 1.4062, -1.4148,  0.5809,  ...,  0.0923,  1.1655, -0.7598],\n",
      "         [ 1.0502, -0.6713,  0.2355,  ...,  0.0590,  0.8764, -1.3657],\n",
      "         ...,\n",
      "         [-0.3568, -0.4274,  1.7607,  ...,  0.1704,  0.1235, -0.1499],\n",
      "         [-0.0811,  0.6301,  0.0406,  ..., -0.7596, -0.2397, -0.1221],\n",
      "         [-0.0219, -0.0584,  0.0052,  ...,  0.0023,  0.0557, -0.0355]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.1540, -0.7875, -0.2857,  ..., -0.0646,  0.2222,  0.5581],\n",
      "         [ 1.4294, -1.1751,  0.7356,  ...,  0.3561,  1.4573, -1.0453],\n",
      "         [ 1.6931, -1.1217,  0.2857,  ...,  0.5971,  1.1531, -1.2755],\n",
      "         ...,\n",
      "         [ 0.0635, -0.4962,  1.5619,  ...,  0.0122,  0.1509,  0.2058],\n",
      "         [ 0.1345,  0.4355,  0.1135,  ..., -0.1961,  0.6263, -0.2299],\n",
      "         [-0.0179, -0.0411,  0.0227,  ...,  0.0144,  0.0125, -0.0410]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.2492, -1.1681, -0.2449,  ...,  0.1231,  0.3600,  0.5647],\n",
      "         [ 1.2719, -1.3478,  0.7083,  ...,  0.2320,  1.2595, -1.0458],\n",
      "         [ 1.5434, -1.0770,  0.4539,  ...,  0.4742,  1.2126, -1.5952],\n",
      "         ...,\n",
      "         [ 0.3677, -1.0958,  1.4703,  ...,  0.1306,  0.4364, -0.2192],\n",
      "         [ 0.2493,  0.4866, -0.1739,  ...,  0.1143,  0.4598, -0.4090],\n",
      "         [ 0.0147, -0.0509, -0.0165,  ...,  0.0034, -0.0185, -0.0409]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 5.7651e-01, -1.0125e+00, -1.0586e-01,  ...,  6.4264e-02,\n",
      "           3.5632e-01,  7.0598e-01],\n",
      "         [ 1.3386e+00, -1.5860e+00,  7.7223e-01,  ...,  4.5542e-01,\n",
      "           1.0579e+00, -1.0779e+00],\n",
      "         [ 1.3815e+00, -1.0805e+00,  8.1563e-02,  ...,  6.3954e-01,\n",
      "           1.1017e+00, -1.2630e+00],\n",
      "         ...,\n",
      "         [ 4.7187e-01, -1.0792e+00,  1.5027e+00,  ...,  2.9703e-01,\n",
      "           2.9404e-01, -5.3835e-02],\n",
      "         [ 1.3777e-01,  5.3669e-01, -8.2492e-01,  ..., -6.4900e-02,\n",
      "           5.6738e-01,  2.0857e-01],\n",
      "         [ 2.3857e-02,  9.4119e-03, -7.2902e-03,  ..., -4.2027e-02,\n",
      "           3.3143e-04, -3.1054e-02]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.5499, -0.5943, -0.1210,  ..., -0.1808,  0.4249,  0.7668],\n",
      "         [ 1.3903, -1.4084,  0.6589,  ...,  0.4669,  0.8726, -0.9791],\n",
      "         [ 1.4993, -0.9469,  0.3105,  ...,  0.2340,  1.1857, -1.0000],\n",
      "         ...,\n",
      "         [ 0.2826, -1.2698,  1.6935,  ...,  0.5412,  0.1310, -0.2577],\n",
      "         [ 0.1495,  1.0336, -0.3711,  ..., -0.5556,  0.4940,  0.3227],\n",
      "         [ 0.0233,  0.0245,  0.0382,  ..., -0.0261, -0.0294, -0.0549]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.2368, -0.5501,  0.1300,  ...,  0.0508,  0.3379,  0.7084],\n",
      "         [ 1.2732, -1.5748,  0.9542,  ..., -0.0704,  0.8218, -0.6951],\n",
      "         [ 1.1649, -0.9484,  0.6973,  ...,  0.0392,  0.5078, -0.6899],\n",
      "         ...,\n",
      "         [ 0.3351, -1.1366,  1.3974,  ...,  0.3476,  0.2494, -0.1416],\n",
      "         [-0.0039,  1.1679, -0.5724,  ..., -0.4160,  0.2132,  0.5861],\n",
      "         [-0.0120, -0.0185,  0.0444,  ..., -0.0427, -0.0318, -0.0299]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0332, -0.6971,  0.0977,  ..., -0.0282, -0.0244,  0.8034],\n",
      "         [ 1.7520, -1.6398,  1.1561,  ..., -0.5655,  0.4014, -0.4589],\n",
      "         [ 1.6835, -1.0502,  0.5238,  ..., -0.2225,  0.3544, -0.6652],\n",
      "         ...,\n",
      "         [ 0.4901, -1.3913,  1.0355,  ...,  0.2770, -0.4135, -0.3452],\n",
      "         [ 0.1698,  0.7205, -0.6651,  ..., -0.3530,  0.3954,  0.5997],\n",
      "         [ 0.0263, -0.2890,  0.2160,  ...,  0.0629, -0.1209, -0.0990]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0880, -0.4159, -0.0073,  ..., -0.1915,  0.2093,  0.4654],\n",
      "         [ 1.6154, -1.5292,  0.6567,  ..., -0.5838,  0.6302, -0.2352],\n",
      "         [ 1.3266, -0.6574,  0.6487,  ..., -0.5888,  0.4744, -0.5298],\n",
      "         ...,\n",
      "         [ 0.5471, -1.2225,  0.7840,  ..., -0.0767, -0.1508, -0.4626],\n",
      "         [-0.1961,  0.6335, -0.6291,  ..., -0.4917,  0.8947,  0.4481],\n",
      "         [ 0.0139, -0.0191,  0.0057,  ...,  0.0338, -0.0330, -0.0266]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 1.1691e-01, -3.9735e-02, -1.0928e-01,  ..., -3.6141e-01,\n",
      "           2.6877e-01,  3.3045e-01],\n",
      "         [ 1.2260e+00, -7.4663e-01,  3.6256e-01,  ...,  1.6007e-01,\n",
      "           7.5153e-01,  2.7402e-01],\n",
      "         [ 1.3149e+00, -2.8815e-01,  4.9387e-01,  ..., -6.4974e-01,\n",
      "          -7.7650e-02, -1.9502e-01],\n",
      "         ...,\n",
      "         [ 1.2370e-01, -9.5042e-01,  3.6132e-01,  ..., -1.1498e-01,\n",
      "          -1.0193e-01, -2.3024e-01],\n",
      "         [-1.2057e-01,  2.0980e-01, -2.9318e-01,  ..., -3.8923e-01,\n",
      "           3.7410e-01,  4.9506e-01],\n",
      "         [ 4.3562e-01,  3.0449e-01,  4.8004e-05,  ...,  6.2245e-02,\n",
      "          -7.3993e-01, -4.2453e-01]]], grad_fn=<NativeLayerNormBackward0>)), past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "bertModel = BertModel.from_pretrained('bert-base-uncased', config=bertconfig)\n",
    "y = bertModel(inputs[\"input_ids\"])\n",
    "print(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "import loralib as lora\n",
    "# Add a pair of low-rank adaptation matrices with rank r=16\n",
    "layer = lora.Linear(512, 512, r=16)\n",
    "lora.mark_only_lora_as_trainable(bertModel)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.sub(input=y[1],other=x[1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}