{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "layer1 = nn.Linear(8,3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1.]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((3,8))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "8"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1.weight.size(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[-0.0581,  0.0361, -0.0649,  0.0933],\n         [ 0.2731, -0.2574, -0.2784, -0.2805],\n         [ 0.2419,  0.2746,  0.2415,  0.3205]], grad_fn=<SliceBackward0>),\n tensor([[-0.3356, -0.2775,  0.2027,  0.0922],\n         [-0.1183,  0.1392, -0.2094,  0.2146],\n         [-0.0479,  0.0785,  0.1698, -0.1927]], grad_fn=<SliceBackward0>))"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1.weight[:,4:], layer1.weight[:,:4]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 1., 1., 0., 0., 0., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0., 0., 0., 0.]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones((2,3))\n",
    "y = torch.zeros((2,6))\n",
    "z = torch.cat((x,y),dim=1)\n",
    "z"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([18, 10]), torch.Size([18, 10]))"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones((18,20))\n",
    "gama, beta = torch.split(x,10,1)\n",
    "gama.shape, beta.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Conada\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294912 || all params: 109777152 || trainable%: 0.2686460658042941\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import BertModel, BertConfig\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "bertconfig = BertConfig.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    ")\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased', config=bertconfig)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "# trainable params: 294912 || all params: 109777152 || trainable%: 0.2686460658042941"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bertModel = BertModel.from_pretrained('bert-base-uncased', config=bertconfig)\n",
    "# y = bertModel(inputs[\"input_ids\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# TARGET_MODULES = [\n",
    "#     \"query\",\n",
    "#     \"v_proj\",\n",
    "# ]\n",
    "\n",
    "# TARGET_MODULES = \"v_proj\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "embeddings\n",
      "embeddings.word_embeddings\n",
      "embeddings.position_embeddings\n",
      "embeddings.token_type_embeddings\n",
      "embeddings.LayerNorm\n",
      "embeddings.dropout\n",
      "encoder\n",
      "encoder.layer\n",
      "encoder.layer.0\n",
      "encoder.layer.0.attention\n",
      "encoder.layer.0.attention.self\n",
      "encoder.layer.0.attention.self.query\n",
      "encoder.layer.0.attention.self.key\n",
      "encoder.layer.0.attention.self.value\n",
      "encoder.layer.0.attention.self.dropout\n",
      "encoder.layer.0.attention.output\n",
      "encoder.layer.0.attention.output.dense\n",
      "encoder.layer.0.attention.output.LayerNorm\n",
      "encoder.layer.0.attention.output.dropout\n",
      "encoder.layer.0.intermediate\n",
      "encoder.layer.0.intermediate.dense\n",
      "encoder.layer.0.intermediate.intermediate_act_fn\n",
      "encoder.layer.0.output\n",
      "encoder.layer.0.output.dense\n",
      "encoder.layer.0.output.LayerNorm\n",
      "encoder.layer.0.output.dropout\n",
      "encoder.layer.1\n",
      "encoder.layer.1.attention\n",
      "encoder.layer.1.attention.self\n",
      "encoder.layer.1.attention.self.query\n",
      "encoder.layer.1.attention.self.key\n",
      "encoder.layer.1.attention.self.value\n",
      "encoder.layer.1.attention.self.dropout\n",
      "encoder.layer.1.attention.output\n",
      "encoder.layer.1.attention.output.dense\n",
      "encoder.layer.1.attention.output.LayerNorm\n",
      "encoder.layer.1.attention.output.dropout\n",
      "encoder.layer.1.intermediate\n",
      "encoder.layer.1.intermediate.dense\n",
      "encoder.layer.1.intermediate.intermediate_act_fn\n",
      "encoder.layer.1.output\n",
      "encoder.layer.1.output.dense\n",
      "encoder.layer.1.output.LayerNorm\n",
      "encoder.layer.1.output.dropout\n",
      "encoder.layer.2\n",
      "encoder.layer.2.attention\n",
      "encoder.layer.2.attention.self\n",
      "encoder.layer.2.attention.self.query\n",
      "encoder.layer.2.attention.self.key\n",
      "encoder.layer.2.attention.self.value\n",
      "encoder.layer.2.attention.self.dropout\n",
      "encoder.layer.2.attention.output\n",
      "encoder.layer.2.attention.output.dense\n",
      "encoder.layer.2.attention.output.LayerNorm\n",
      "encoder.layer.2.attention.output.dropout\n",
      "encoder.layer.2.intermediate\n",
      "encoder.layer.2.intermediate.dense\n",
      "encoder.layer.2.intermediate.intermediate_act_fn\n",
      "encoder.layer.2.output\n",
      "encoder.layer.2.output.dense\n",
      "encoder.layer.2.output.LayerNorm\n",
      "encoder.layer.2.output.dropout\n",
      "encoder.layer.3\n",
      "encoder.layer.3.attention\n",
      "encoder.layer.3.attention.self\n",
      "encoder.layer.3.attention.self.query\n",
      "encoder.layer.3.attention.self.key\n",
      "encoder.layer.3.attention.self.value\n",
      "encoder.layer.3.attention.self.dropout\n",
      "encoder.layer.3.attention.output\n",
      "encoder.layer.3.attention.output.dense\n",
      "encoder.layer.3.attention.output.LayerNorm\n",
      "encoder.layer.3.attention.output.dropout\n",
      "encoder.layer.3.intermediate\n",
      "encoder.layer.3.intermediate.dense\n",
      "encoder.layer.3.intermediate.intermediate_act_fn\n",
      "encoder.layer.3.output\n",
      "encoder.layer.3.output.dense\n",
      "encoder.layer.3.output.LayerNorm\n",
      "encoder.layer.3.output.dropout\n",
      "encoder.layer.4\n",
      "encoder.layer.4.attention\n",
      "encoder.layer.4.attention.self\n",
      "encoder.layer.4.attention.self.query\n",
      "encoder.layer.4.attention.self.key\n",
      "encoder.layer.4.attention.self.value\n",
      "encoder.layer.4.attention.self.dropout\n",
      "encoder.layer.4.attention.output\n",
      "encoder.layer.4.attention.output.dense\n",
      "encoder.layer.4.attention.output.LayerNorm\n",
      "encoder.layer.4.attention.output.dropout\n",
      "encoder.layer.4.intermediate\n",
      "encoder.layer.4.intermediate.dense\n",
      "encoder.layer.4.intermediate.intermediate_act_fn\n",
      "encoder.layer.4.output\n",
      "encoder.layer.4.output.dense\n",
      "encoder.layer.4.output.LayerNorm\n",
      "encoder.layer.4.output.dropout\n",
      "encoder.layer.5\n",
      "encoder.layer.5.attention\n",
      "encoder.layer.5.attention.self\n",
      "encoder.layer.5.attention.self.query\n",
      "encoder.layer.5.attention.self.key\n",
      "encoder.layer.5.attention.self.value\n",
      "encoder.layer.5.attention.self.dropout\n",
      "encoder.layer.5.attention.output\n",
      "encoder.layer.5.attention.output.dense\n",
      "encoder.layer.5.attention.output.LayerNorm\n",
      "encoder.layer.5.attention.output.dropout\n",
      "encoder.layer.5.intermediate\n",
      "encoder.layer.5.intermediate.dense\n",
      "encoder.layer.5.intermediate.intermediate_act_fn\n",
      "encoder.layer.5.output\n",
      "encoder.layer.5.output.dense\n",
      "encoder.layer.5.output.LayerNorm\n",
      "encoder.layer.5.output.dropout\n",
      "encoder.layer.6\n",
      "encoder.layer.6.attention\n",
      "encoder.layer.6.attention.self\n",
      "encoder.layer.6.attention.self.query\n",
      "encoder.layer.6.attention.self.key\n",
      "encoder.layer.6.attention.self.value\n",
      "encoder.layer.6.attention.self.dropout\n",
      "encoder.layer.6.attention.output\n",
      "encoder.layer.6.attention.output.dense\n",
      "encoder.layer.6.attention.output.LayerNorm\n",
      "encoder.layer.6.attention.output.dropout\n",
      "encoder.layer.6.intermediate\n",
      "encoder.layer.6.intermediate.dense\n",
      "encoder.layer.6.intermediate.intermediate_act_fn\n",
      "encoder.layer.6.output\n",
      "encoder.layer.6.output.dense\n",
      "encoder.layer.6.output.LayerNorm\n",
      "encoder.layer.6.output.dropout\n",
      "encoder.layer.7\n",
      "encoder.layer.7.attention\n",
      "encoder.layer.7.attention.self\n",
      "encoder.layer.7.attention.self.query\n",
      "encoder.layer.7.attention.self.key\n",
      "encoder.layer.7.attention.self.value\n",
      "encoder.layer.7.attention.self.dropout\n",
      "encoder.layer.7.attention.output\n",
      "encoder.layer.7.attention.output.dense\n",
      "encoder.layer.7.attention.output.LayerNorm\n",
      "encoder.layer.7.attention.output.dropout\n",
      "encoder.layer.7.intermediate\n",
      "encoder.layer.7.intermediate.dense\n",
      "encoder.layer.7.intermediate.intermediate_act_fn\n",
      "encoder.layer.7.output\n",
      "encoder.layer.7.output.dense\n",
      "encoder.layer.7.output.LayerNorm\n",
      "encoder.layer.7.output.dropout\n",
      "encoder.layer.8\n",
      "encoder.layer.8.attention\n",
      "encoder.layer.8.attention.self\n",
      "encoder.layer.8.attention.self.query\n",
      "encoder.layer.8.attention.self.key\n",
      "encoder.layer.8.attention.self.value\n",
      "encoder.layer.8.attention.self.dropout\n",
      "encoder.layer.8.attention.output\n",
      "encoder.layer.8.attention.output.dense\n",
      "encoder.layer.8.attention.output.LayerNorm\n",
      "encoder.layer.8.attention.output.dropout\n",
      "encoder.layer.8.intermediate\n",
      "encoder.layer.8.intermediate.dense\n",
      "encoder.layer.8.intermediate.intermediate_act_fn\n",
      "encoder.layer.8.output\n",
      "encoder.layer.8.output.dense\n",
      "encoder.layer.8.output.LayerNorm\n",
      "encoder.layer.8.output.dropout\n",
      "encoder.layer.9\n",
      "encoder.layer.9.attention\n",
      "encoder.layer.9.attention.self\n",
      "encoder.layer.9.attention.self.query\n",
      "encoder.layer.9.attention.self.key\n",
      "encoder.layer.9.attention.self.value\n",
      "encoder.layer.9.attention.self.dropout\n",
      "encoder.layer.9.attention.output\n",
      "encoder.layer.9.attention.output.dense\n",
      "encoder.layer.9.attention.output.LayerNorm\n",
      "encoder.layer.9.attention.output.dropout\n",
      "encoder.layer.9.intermediate\n",
      "encoder.layer.9.intermediate.dense\n",
      "encoder.layer.9.intermediate.intermediate_act_fn\n",
      "encoder.layer.9.output\n",
      "encoder.layer.9.output.dense\n",
      "encoder.layer.9.output.LayerNorm\n",
      "encoder.layer.9.output.dropout\n",
      "encoder.layer.10\n",
      "encoder.layer.10.attention\n",
      "encoder.layer.10.attention.self\n",
      "encoder.layer.10.attention.self.query\n",
      "encoder.layer.10.attention.self.key\n",
      "encoder.layer.10.attention.self.value\n",
      "encoder.layer.10.attention.self.dropout\n",
      "encoder.layer.10.attention.output\n",
      "encoder.layer.10.attention.output.dense\n",
      "encoder.layer.10.attention.output.LayerNorm\n",
      "encoder.layer.10.attention.output.dropout\n",
      "encoder.layer.10.intermediate\n",
      "encoder.layer.10.intermediate.dense\n",
      "encoder.layer.10.intermediate.intermediate_act_fn\n",
      "encoder.layer.10.output\n",
      "encoder.layer.10.output.dense\n",
      "encoder.layer.10.output.LayerNorm\n",
      "encoder.layer.10.output.dropout\n",
      "encoder.layer.11\n",
      "encoder.layer.11.attention\n",
      "encoder.layer.11.attention.self\n",
      "encoder.layer.11.attention.self.query\n",
      "encoder.layer.11.attention.self.key\n",
      "encoder.layer.11.attention.self.value\n",
      "encoder.layer.11.attention.self.dropout\n",
      "encoder.layer.11.attention.output\n",
      "encoder.layer.11.attention.output.dense\n",
      "encoder.layer.11.attention.output.LayerNorm\n",
      "encoder.layer.11.attention.output.dropout\n",
      "encoder.layer.11.intermediate\n",
      "encoder.layer.11.intermediate.dense\n",
      "encoder.layer.11.intermediate.intermediate_act_fn\n",
      "encoder.layer.11.output\n",
      "encoder.layer.11.output.dense\n",
      "encoder.layer.11.output.LayerNorm\n",
      "encoder.layer.11.output.dropout\n",
      "pooler\n",
      "pooler.dense\n",
      "pooler.activation\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "key_list = [key for key, _ in bertModel.named_modules()]\n",
    "for i in list(range(len(key_list))):\n",
    "     # target_module_found = any(key_list[i].endswith(target_key) for target_key in TARGET_MODULES)\n",
    "     # if target_module_found:\n",
    "    print(key_list[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "import loralib as lora\n",
    "# Add a pair of low-rank adaptation matrices with rank r=16\n",
    "layer = lora.Linear(512, 512, r=16)\n",
    "lora.mark_only_lora_as_trainable(bertModel)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.sub(input=y[1],other=x[1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Config' object has no attribute 'inference_mode'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36m<cell line: 8>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      6\u001B[0m bertconfig \u001B[38;5;241m=\u001B[39m BertConfig\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbert-base-uncased\u001B[39m\u001B[38;5;124m'\u001B[39m, output_hidden_states\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      7\u001B[0m model \u001B[38;5;241m=\u001B[39m BertModel\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbert-base-uncased\u001B[39m\u001B[38;5;124m'\u001B[39m, config\u001B[38;5;241m=\u001B[39mbertconfig)\n\u001B[1;32m----> 8\u001B[0m PEFTmodel \u001B[38;5;241m=\u001B[39m \u001B[43mLoraModel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\ThreeOrMore_OGM-GE\\models\\lora.py:120\u001B[0m, in \u001B[0;36mLoraModel.__init__\u001B[1;34m(self, config, model)\u001B[0m\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpeft_config \u001B[38;5;241m=\u001B[39m config\n\u001B[0;32m    119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m model\n\u001B[1;32m--> 120\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_find_and_replace\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# 找到并替换关键层\u001B[39;00m\n\u001B[0;32m    121\u001B[0m mark_only_lora_as_trainable(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpeft_config\u001B[38;5;241m.\u001B[39mbias) \u001B[38;5;66;03m# 保留lora参数 其他固定\u001B[39;00m\n\u001B[0;32m    122\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mforward\n",
      "File \u001B[1;32mD:\\ThreeOrMore_OGM-GE\\models\\lora.py:137\u001B[0m, in \u001B[0;36mLoraModel._find_and_replace\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    127\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[0;32m    128\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTo use Lora with 8-bit quantization, please install the `bitsandbytes` package. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    129\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou can install it with `pip install bitsandbytes`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    130\u001B[0m     )\n\u001B[0;32m    131\u001B[0m is_target_modules_in_base_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    132\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    133\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpeft_config\u001B[38;5;241m.\u001B[39mr,\n\u001B[0;32m    134\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlora_alpha\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpeft_config\u001B[38;5;241m.\u001B[39mlora_alpha,\n\u001B[0;32m    135\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlora_dropout\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpeft_config\u001B[38;5;241m.\u001B[39mlora_dropout,\n\u001B[0;32m    136\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfan_in_fan_out\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpeft_config\u001B[38;5;241m.\u001B[39mfan_in_fan_out,\n\u001B[1;32m--> 137\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmerge_weights\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpeft_config\u001B[38;5;241m.\u001B[39mmerge_weights \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpeft_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minference_mode\u001B[49m,\n\u001B[0;32m    138\u001B[0m }\n\u001B[0;32m    139\u001B[0m key_list \u001B[38;5;241m=\u001B[39m [key \u001B[38;5;28;01mfor\u001B[39;00m key, _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mnamed_modules()]\n\u001B[0;32m    140\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m key_list:\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'Config' object has no attribute 'inference_mode'"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertConfig\n",
    "from models.lora import *\n",
    "from config import *\n",
    "\n",
    "config = Config()\n",
    "bertconfig = BertConfig.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "model = BertModel.from_pretrained('bert-base-uncased', config=bertconfig)\n",
    "PEFTmodel = LoraModel(config=config,model=model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mosei\n"
     ]
    }
   ],
   "source": [
    "x = 'MOSEI'\n",
    "print(str(x).lower())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5e-06\n"
     ]
    }
   ],
   "source": [
    "x = 3.5e-6\n",
    "print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}